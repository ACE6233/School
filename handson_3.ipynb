{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ACE6233/School/blob/main/handson_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR2vA3DKfU5Y"
      },
      "source": [
        "## Hands-on 3\n",
        "#### The objective of this hands-on is to train a convolutional neural network (CNN) to recognize the handwritten digits (0 - 9)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tLf-R5DfU5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002e0d60-2a85-4be1-b5a2-3bf30d885b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.44.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTGh9k866VHq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "bed8eb14-3e71-4659-e8ef-2f5a8e8651c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# plot 4 images as gray scale\n",
        "sp1 = plt.subplot(141)\n",
        "sp1.axis(False)\n",
        "sp1.set_title(y_train[0])\n",
        "sp1.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
        "sp2 = plt.subplot(142)\n",
        "sp2.axis(False)\n",
        "sp2.set_title(y_train[1])\n",
        "sp2.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
        "sp3 = plt.subplot(143)\n",
        "sp3.axis(False)\n",
        "sp3.set_title(y_train[2])\n",
        "sp3.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
        "sp4 = plt.subplot(144)\n",
        "sp4.axis(False)\n",
        "sp4.set_title(y_train[3])\n",
        "sp4.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
        "# show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44LWqaCsfU5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48156da-dd0d-438e-a816-9788fc2b2f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9152 - loss: 0.2781 - val_accuracy: 0.9876 - val_loss: 0.0404\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9887 - loss: 0.0369 - val_accuracy: 0.9854 - val_loss: 0.0437\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9916 - loss: 0.0252 - val_accuracy: 0.9856 - val_loss: 0.0413\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9950 - loss: 0.0169 - val_accuracy: 0.9903 - val_loss: 0.0308\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9965 - loss: 0.0114 - val_accuracy: 0.9925 - val_loss: 0.0283\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9972 - loss: 0.0087 - val_accuracy: 0.9918 - val_loss: 0.0296\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9969 - loss: 0.0097 - val_accuracy: 0.9900 - val_loss: 0.0368\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9971 - loss: 0.0085 - val_accuracy: 0.9920 - val_loss: 0.0354\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9981 - loss: 0.0064 - val_accuracy: 0.9933 - val_loss: 0.0289\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9983 - loss: 0.0054 - val_accuracy: 0.9920 - val_loss: 0.0378\n",
            "Error: 0.80%\n",
            "Saved model to disk\n"
          ]
        }
      ],
      "source": [
        "# reshape to be [samples][channels][width][height]\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# define model for LeNet\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(20, (5, 5), padding=\"same\", input_shape=(28, 28, 1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(50, (5, 5), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Error: %.2f%%\" % (100-scores[1]*100))\n",
        "\n",
        "# Insert the codes to serialize model to JSON\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.weights.h5\")\n",
        "print(\"Saved model to disk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhKJygVUfU5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "c0527768-b2c3-4015-d6d8-a1525ffd31dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter an index between 0 and 9999: 109\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 507ms/step\n",
            "[[8.5172342e-19 3.1589795e-16 1.2422078e-19 2.0905665e-21 1.0000000e+00\n",
            "  2.8365327e-17 8.6320653e-14 3.1209734e-16 6.7525667e-13 2.8608449e-15]]\n",
            "Predicted label: 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Make predition (change index to a number between 0 and 9999)\n",
        "try:\n",
        "  index = int(input(\"Enter an index between 0 and 9999: \"))\n",
        "  assert 0 <= index < 10000, \"The index should be between 0 and 9999\"\n",
        "except Exception as e:\n",
        "  print(\"Error: \", e)\n",
        "else:\n",
        "  res = model.predict(X_test[index].reshape(1, 28, 28, 1))\n",
        "  print(res)\n",
        "  print(f'Predicted label: {np.argmax(res)}')\n",
        "\n",
        "  # Display the test image and show the actual label\n",
        "  plt.axis(False)\n",
        "  plt.title(f'Actual label: {y_test[index].argmax()}')\n",
        "  plt.imshow(X_test[index].reshape(28, 28) * 255, cmap='gray')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeKYMrXqr81F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "outputId": "26c04ae6-3d70-45da-e1a4-5b4906967e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "Gradio Version: 5.44.1\n",
            "OpenCV Version: 4.12.0\n",
            "Keras Model loaded successfully from disk.\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ac4bad285e79a19551.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ac4bad285e79a19551.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "Received input type: <class 'dict'>\n",
            "Received input value (dict keys): dict_keys(['background', 'layers', 'composite'])\n",
            "--------------------\n",
            "Input is a dictionary. Keys: dict_keys(['background', 'layers', 'composite'])\n",
            "Found 'composite' key with numpy array, using it.\n",
            "Processing image with shape: (800, 800)\n",
            "Image is grayscale.\n",
            "Inverting image colors (assuming white on black needed).\n",
            "Applying threshold.\n",
            "Resizing image to (28, 28).\n",
            "Reshaping and normalizing for model.\n",
            "Making prediction.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617ms/step\n",
            "Formatting predictions.\n",
            "Returning results.\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import cv2 # Using OpenCV for image processing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import model_from_json\n",
        "import os # To check if model files exist\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "print(\"Gradio Version:\", gr.__version__)\n",
        "print(\"OpenCV Version:\", cv2.__version__)\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_JSON_PATH = 'model.json'\n",
        "MODEL_WEIGHTS_PATH = 'model.weights.h5'\n",
        "INPUT_SHAPE = (28, 28) # Expected input shape for the model (height, width)\n",
        "\n",
        "# --- Load Model (Do this once outside the prediction function) ---\n",
        "net = None\n",
        "model_loaded = False\n",
        "\n",
        "if os.path.exists(MODEL_JSON_PATH) and os.path.exists(MODEL_WEIGHTS_PATH):\n",
        "    try:\n",
        "        # Load model architecture from JSON\n",
        "        with open(MODEL_JSON_PATH, 'r') as json_file:\n",
        "            loaded_model_json = json_file.read()\n",
        "        net = model_from_json(loaded_model_json)\n",
        "\n",
        "        # Load weights into the model\n",
        "        net.load_weights(MODEL_WEIGHTS_PATH)\n",
        "\n",
        "        # Compile the model (often necessary after loading, even if just for prediction)\n",
        "        # Use standard settings; adjust if your model requires specific ones.\n",
        "        net.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        print(\"Keras Model loaded successfully from disk.\")\n",
        "        model_loaded = True\n",
        "        # Optional: Print model summary\n",
        "        # net.summary()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Keras model: {e}\")\n",
        "        print(\"Please ensure 'model.json' and 'model.weights.h5' are uploaded correctly.\")\n",
        "else:\n",
        "    print(f\"Error: Model files not found.\")\n",
        "    print(f\"Please make sure '{MODEL_JSON_PATH}' and '{MODEL_WEIGHTS_PATH}' exist in the current directory.\")\n",
        "\n",
        "# --- Prediction Function ---\n",
        "def recognize_digit(image):\n",
        "    # --- START DEBUGGING PRINTS ---\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Received input type: {type(image)}\")\n",
        "    # Limit printing the full value if it's large (like an image array or complex dict)\n",
        "    if isinstance(image, (dict, np.ndarray)) and image is not None:\n",
        "         # Try printing keys for dict, shape for array, else snippet\n",
        "         if isinstance(image, dict):\n",
        "              print(f\"Received input value (dict keys): {image.keys()}\")\n",
        "         elif hasattr(image, 'shape'):\n",
        "              print(f\"Received input value (array shape): {image.shape}\")\n",
        "         else:\n",
        "              print(f\"Received input value: {str(image)[:100]}...\") # Print snippet\n",
        "    else:\n",
        "         print(f\"Received input value: {image}\") # Print simple types directly\n",
        "    print(\"-\" * 20)\n",
        "    # --- END DEBUGGING PRINTS ---\n",
        "\n",
        "    # Handle None case FIRST\n",
        "    if image is None:\n",
        "        print(\"Input image is None, returning.\")\n",
        "        return \"Please draw a digit.\", None\n",
        "\n",
        "    # --- Check if it's a dictionary and try to extract 'composite' image ---\n",
        "    if isinstance(image, dict):\n",
        "        print(\"Input is a dictionary. Keys:\", image.keys())\n",
        "        # --- Updated Logic: Check for 'composite' key ---\n",
        "        if 'composite' in image and isinstance(image['composite'], np.ndarray):\n",
        "            print(\"Found 'composite' key with numpy array, using it.\")\n",
        "            image = image['composite'] # Extract the composite image array\n",
        "        else:\n",
        "            # If 'composite' key doesn't exist or isn't a numpy array\n",
        "            print(\"Error: Input is a dictionary but cannot find valid 'composite' image data.\")\n",
        "            # Print types of dictionary values for more detail if needed\n",
        "            for key in image:\n",
        "                 if image[key] is not None:\n",
        "                      print(f\"  Key '{key}' type: {type(image[key])}, Shape/Len: {getattr(image[key], 'shape', len(getattr(image[key], '__dict__', image[key])))}\")\n",
        "                 else:\n",
        "                      print(f\"  Key '{key}' type: None\")\n",
        "            return \"Error: Unexpected dictionary format from Sketchpad.\", None\n",
        "    # --- End of dictionary handling ---\n",
        "\n",
        "    # --- Defensive Check for shape attribute ---\n",
        "    # Now, 'image' should be a NumPy array (either originally or extracted)\n",
        "    if not hasattr(image, 'shape'):\n",
        "         print(f\"Error: Input STILL does not have 'shape' after processing. Type is {type(image)}\")\n",
        "         return \"Error: Unexpected input format after processing.\", None\n",
        "\n",
        "    # Check image dimensions and convert to grayscale if necessary\n",
        "    print(f\"Processing image with shape: {image.shape}\")\n",
        "    if len(image.shape) == 2: # Grayscale already\n",
        "        gray_image = image\n",
        "        print(\"Image is grayscale.\")\n",
        "    elif len(image.shape) == 3 and image.shape[2] == 4: # RGBA\n",
        "        # Sketchpad composite might be RGBA\n",
        "        print(\"Image is RGBA, converting to grayscale.\")\n",
        "        # Convert RGBA to Grayscale - handle transparency appropriately if needed\n",
        "        # A simple approach is to convert to RGB first, then Gray\n",
        "        # Or directly, ensuring background color handling if alpha is used\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGBA2GRAY) # OpenCV handles RGBA->Gray\n",
        "    elif len(image.shape) == 3 and image.shape[2] == 3: # RGB\n",
        "        print(\"Image is RGB, converting to grayscale.\")\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        print(f\"Error: Input image has unexpected shape: {image.shape}\")\n",
        "        return \"Error: Invalid image dimensions.\", None\n",
        "\n",
        "    # Ensure manual inversion is still present if needed (white-on-black)\n",
        "    print(\"Inverting image colors (assuming white on black needed).\")\n",
        "    gray_image = 255 - gray_image\n",
        "\n",
        "    # Optional: Apply thresholding\n",
        "    print(\"Applying threshold.\")\n",
        "    _, processed_image = cv2.threshold(gray_image, 50, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Resize to the target input size\n",
        "    print(f\"Resizing image to {INPUT_SHAPE}.\")\n",
        "    processed_image = cv2.resize(processed_image, INPUT_SHAPE, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Reshape and Normalize\n",
        "    print(\"Reshaping and normalizing for model.\")\n",
        "    blob = processed_image.reshape(1, INPUT_SHAPE[0], INPUT_SHAPE[1], 1).astype('float32')\n",
        "    blob /= 255.0\n",
        "\n",
        "    # Make Prediction\n",
        "    print(\"Making prediction.\")\n",
        "    try:\n",
        "        predictions = net.predict(blob)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        return f\"Prediction error: {e}\", processed_image # Return error and processed image\n",
        "\n",
        "    # Format Output\n",
        "    print(\"Formatting predictions.\")\n",
        "    confidences = {str(i): float(predictions[0][i]) for i in range(len(predictions[0]))}\n",
        "\n",
        "    print(\"Returning results.\")\n",
        "    return confidences, processed_image\n",
        "\n",
        "# --- Create Gradio Interface ---\n",
        "if model_loaded:\n",
        "# Define Input and Output Components\n",
        "    input_sketchpad = gr.Sketchpad(\n",
        "        label=\"Draw a Digit (0-9)\",\n",
        "        image_mode='L'\n",
        "    )\n",
        "\n",
        "    output_label = gr.Label(\n",
        "        num_top_classes=3,\n",
        "        label=\"Predictions\"\n",
        "    )\n",
        "\n",
        "    # Corrected gr.Image definition\n",
        "    output_processed_image = gr.Image(\n",
        "        label=\"Processed Input (28x28)\",\n",
        "        # shape=(INPUT_SHAPE[1], INPUT_SHAPE[0]), # This caused the error\n",
        "        width=INPUT_SHAPE[1],  # Set width explicitly (should be 28)\n",
        "        height=INPUT_SHAPE[0], # Set height explicitly (should be 28)\n",
        "        image_mode='L'         # Keep image mode as grayscale\n",
        "    )\n",
        "\n",
        "    # Create the Interface\n",
        "    iface = gr.Interface(\n",
        "        fn=recognize_digit,\n",
        "        inputs=input_sketchpad,\n",
        "        outputs=[output_label, output_processed_image], # List of outputs\n",
        "        live=False,\n",
        "        title=\"Handwritten Digit Recognizer\",\n",
        "        description=\"Draw a single digit (0-9) in the box below. The model will predict the digit in real-time. Ensure the model files ('model.json', 'model.weights.h5') are uploaded.\"\n",
        "    )\n",
        "\n",
        "    # Launch the Interface (share=True provides a public link for Colab)\n",
        "    iface.launch(share=True, debug=True) # debug=True can help troubleshoot\n",
        "\n",
        "else:\n",
        "    print(\"\\n--- Gradio Interface cannot start because the model failed to load. ---\")\n",
        "    print(\"Please check the file paths and ensure the model files are correct and uploaded.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}